#!/usr/bin/env python3
"""
V2M4ç®—æ³•ç»Ÿä¸€æµ‹è¯•è„šæœ¬
åˆå¹¶å®Œæ•´ç®—æ³•æµ‹è¯•å’Œå¯è§†åŒ–åŠŸèƒ½æµ‹è¯•ï¼Œæ”¯æŒå‚æ•°æ§åˆ¶
"""

import sys
import time
import os
import argparse
from pathlib import Path
from typing import List, Dict, Optional

# è®¾ç½®CUDAè®¾å¤‡
def set_cuda_device(device_id: int = 2):
    """è®¾ç½®CUDAè®¾å¤‡"""
    os.environ['CUDA_VISIBLE_DEVICES'] = str(device_id)
    print(f"ğŸ”§ è®¾ç½®CUDAè®¾å¤‡: {device_id}")

def test_environment():
    """æµ‹è¯•ç¯å¢ƒæ£€æŸ¥"""
    print("ğŸ”§ ç¯å¢ƒæ£€æŸ¥...")
    
    # CUDAæ£€æŸ¥
    print(f"   ğŸ¯ CUDAè®¾å¤‡è®¾ç½®: {os.environ.get('CUDA_VISIBLE_DEVICES', 'default')}")
    
    try:
        import torch
        if torch.cuda.is_available():
            print(f"   âœ… CUDAå¯ç”¨ï¼Œå½“å‰è®¾å¤‡: {torch.cuda.current_device()}")
            print(f"   ğŸ“Š GPUåç§°: {torch.cuda.get_device_name()}")
            print(f"   ğŸ’¾ GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
        else:
            print("   âš ï¸ CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPU")
    except ImportError:
        print("   âŒ PyTorchæœªå®‰è£…")
        return False
    
    # åŒ…å¯¼å…¥æ£€æŸ¥
    try:
        import camera_search
        print("   âœ… camera_searchåŒ…å¯¼å…¥æˆåŠŸ")
    except ImportError as e:
        print(f"   âŒ camera_searchåŒ…å¯¼å…¥å¤±è´¥: {e}")
        return False
    
    # å¿…éœ€ä¾èµ–æ£€æŸ¥
    try:
        import nvdiffrast.torch as dr
        print("   âœ… nvdiffrastå¯ç”¨ (å¿…éœ€)")
    except ImportError:
        print("   âŒ nvdiffrastä¸å¯ç”¨ (å¿…éœ€ä¾èµ–)")
        return False
    
    try:
        import kiui
        print("   âœ… kiuiå¯ç”¨ (å¿…éœ€)")
    except ImportError:
        print("   âŒ kiuiä¸å¯ç”¨ (å¿…éœ€ä¾èµ–)")
        return False
    
    # å¯è§†åŒ–æ£€æŸ¥
    try:
        import matplotlib.pyplot as plt
        print("   âœ… matplotlibå¯ç”¨")
    except ImportError:
        print("   âš ï¸ matplotlibä¸å¯ç”¨ï¼Œå¯è§†åŒ–åŠŸèƒ½å°†å—é™")
    
    # æ•°æ®å®Œæ•´æ€§æ£€æŸ¥
    try:
        from camera_search import validate_data_integrity
        validation = validate_data_integrity()
        print(f"   ğŸ“Š æ•°æ®å®Œæ•´æ€§: {validation['data_completeness']:.1f}%")
        print(f"   ğŸ“ æœ‰æ•ˆæ•°æ®å¯¹: {validation['valid_data_pairs']}ä¸ª")
        
        if validation['valid_data_pairs'] == 0:
            print("   âŒ æ²¡æœ‰æœ‰æ•ˆæ•°æ®å¯¹ï¼Œæ— æ³•è¿›è¡Œæµ‹è¯•")
            return False
            
    except Exception as e:
        print(f"   âš ï¸ æ•°æ®æ£€æŸ¥å¤±è´¥: {e}")
    
    return True

def test_visualization_components():
    """æµ‹è¯•å¯è§†åŒ–ç»„ä»¶åŠŸèƒ½ï¼ˆä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ï¼‰"""
    print("ğŸ¨ æµ‹è¯•å¯è§†åŒ–ç»„ä»¶...")
    
    try:
        from camera_search.visualization import V2M4Visualizer
        from camera_search import DataPair, CameraPose
        import numpy as np
        import cv2
        
        # åˆ›å»ºå¯è§†åŒ–å™¨
        visualizer = V2M4Visualizer("outputs/test_visualization")
        print("   âœ… å¯è§†åŒ–å™¨åˆ›å»ºæˆåŠŸ")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_scene = "1"
        data_pair = DataPair.from_scene_name(test_scene)
        
        if not data_pair.exists():
            print("   âš ï¸ æµ‹è¯•åœºæ™¯ä¸å­˜åœ¨ï¼Œè·³è¿‡ç»„ä»¶æµ‹è¯•")
            return True
        
        # åŠ è½½æµ‹è¯•å›¾åƒ
        reference_image = cv2.imread(data_pair.image_path)
        reference_image = cv2.cvtColor(reference_image, cv2.COLOR_BGR2RGB)
        
        # åˆ›å»ºæ¨¡æ‹Ÿæ¸²æŸ“ç»“æœ
        rendered_result = reference_image.copy()
        noise = np.random.randint(-20, 20, rendered_result.shape, dtype=np.int16)
        rendered_result = np.clip(rendered_result.astype(np.int16) + noise, 0, 255).astype(np.uint8)
        
        # åˆ›å»ºæµ‹è¯•å§¿æ€
        test_pose = CameraPose(elevation=15.0, azimuth=45.0, radius=2.5)
        
        # æµ‹è¯•ç»“æœå¯¹æ¯”å›¾
        comparison_path = visualizer.create_result_comparison(
            data_pair=data_pair,
            reference_image=reference_image,
            rendered_result=rendered_result,
            final_pose=test_pose,
            mesh_info={'vertices_count': 1000, 'faces_count': 2000, 'scale': 1.5},
            algorithm_stats={'initial_samples': 128, 'top_n': 7, 'final_score': 0.85},
            execution_time=120.5
        )
        print(f"   âœ… ç»“æœå¯¹æ¯”å›¾: {Path(comparison_path).name}")
        
        # æµ‹è¯•ä¼˜åŒ–è¿‡ç¨‹å¯è§†åŒ–
        progression_data = [
            {
                'step_name': 'Initial',
                'pose': CameraPose(elevation=0, azimuth=0, radius=3.0),
                'rendered_image': rendered_result,
                'similarity': 0.6,
                'score': 0.6
            },
            {
                'step_name': 'Final',
                'pose': test_pose,
                'rendered_image': rendered_result,
                'similarity': 0.85,
                'score': 0.85
            }
        ]
        
        progression_path = visualizer.create_pose_progression_visualization(
            data_pair=data_pair,
            reference_image=reference_image,
            progression_data=progression_data,
            final_pose=test_pose
        )
        print(f"   âœ… ä¼˜åŒ–è¿‡ç¨‹å›¾: {Path(progression_path).name}")
        
        return True
        
    except Exception as e:
        print(f"   âŒ å¯è§†åŒ–ç»„ä»¶æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_single_scene(scene_name: str, enable_visualization: bool = True, device: str = "cuda", use_vggt: bool = False) -> Optional[Dict]:
    """æµ‹è¯•å•ä¸ªåœºæ™¯çš„å®Œæ•´V2M4ç®—æ³•"""
    model_name = "VGGT" if use_vggt else "DUSt3R"
    print(f"ğŸ¬ æµ‹è¯•åœºæ™¯: {scene_name} (ä½¿ç”¨{model_name})")
    
    try:
        from camera_search.core import CleanV2M4CameraSearch, DataPair
        
        # åˆ›å»ºæ•°æ®å¯¹
        data_pair = DataPair.from_scene_name(scene_name)
        if not data_pair.exists():
            print(f"   âŒ åœºæ™¯æ•°æ®ä¸å­˜åœ¨: {scene_name}")
            return None
        
        # åˆ›å»ºæœç´¢å™¨
        searcher = CleanV2M4CameraSearch(
            dust3r_model_path="models/dust3r/DUSt3R_ViTLarge_BaseDecoder_512_dpt",
            device=device,
            enable_visualization=enable_visualization
        )
        
        # é…ç½®æ¨¡å‹
        if use_vggt:
            searcher.config['use_vggt'] = True
            searcher.config['model_name'] = 'vggt'
            print(f"   ğŸ”„ åˆ‡æ¢åˆ°VGGTæ¨¡å¼")
        
        # è¿è¡Œå®Œæ•´çš„V2M4ç®—æ³•
        start_time = time.time()
        
        best_pose = searcher.search_camera_pose(
            data_pair=data_pair,
            save_visualization=enable_visualization
        )
        
        elapsed = time.time() - start_time
        
        if best_pose is not None:
            result = {
                'scene_name': scene_name,
                'pose': best_pose,
                'execution_time': elapsed,
                'success': True,
                'model': model_name
            }
            
            print(f"   âœ… æˆåŠŸ! å§¿æ€: ä»°è§’={best_pose.elevation:.1f}Â°, æ–¹ä½è§’={best_pose.azimuth:.1f}Â°, è·ç¦»={best_pose.radius:.2f}")
            print(f"   â±ï¸ è€—æ—¶: {elapsed:.1f}ç§’")
            print(f"   ğŸ¤– æ¨¡å‹: {model_name}")
            
            if enable_visualization:
                # æ£€æŸ¥å¯è§†åŒ–æ–‡ä»¶
                output_dir = Path("outputs/visualization")
                if output_dir.exists():
                    viz_files = list(output_dir.glob(f"*{scene_name}*"))
                    print(f"   ğŸ“Š å¯è§†åŒ–æ–‡ä»¶: {len(viz_files)}ä¸ª")
            
            return result
        else:
            print(f"   âŒ ç®—æ³•æ‰§è¡Œå¤±è´¥")
            return {
                'scene_name': scene_name,
                'pose': None,
                'execution_time': elapsed,
                'success': False,
                'model': model_name
            }
            
    except Exception as e:
        print(f"   âŒ æµ‹è¯•å¤±è´¥: {e}")
        return {
            'scene_name': scene_name,
            'pose': None,
            'execution_time': 0,
            'success': False,
            'error': str(e),
            'model': model_name
        }

def test_multiple_scenes(
    num_scenes: int = 3, 
    enable_visualization: bool = True,
    create_batch_summary: bool = True,
    device: str = "cuda",
    use_vggt: bool = False
) -> Dict:
    """æµ‹è¯•å¤šä¸ªåœºæ™¯çš„æ‰¹é‡å¤„ç†"""
    model_name = "VGGT" if use_vggt else "DUSt3R"
    print(f"\nğŸ”„ æ‰¹é‡æµ‹è¯• {num_scenes} ä¸ªåœºæ™¯ (ä½¿ç”¨{model_name})...")
    print(f"   ğŸ¨ å¯è§†åŒ–: {'å¯ç”¨' if enable_visualization else 'ç¦ç”¨'}")
    print(f"   ğŸ“‹ æ‰¹é‡æ€»ç»“: {'å¯ç”¨' if create_batch_summary else 'ç¦ç”¨'}")
    
    try:
        from camera_search.core import DataManager
        
        # å‘ç°å¯ç”¨åœºæ™¯
        data_manager = DataManager()
        available_data_pairs = data_manager.discover_data_pairs()
        available_scenes = [dp.scene_name for dp in available_data_pairs]
        print(f"   ğŸ“ å‘ç°åœºæ™¯: {len(available_scenes)}ä¸ª")
        
        # é€‰æ‹©æµ‹è¯•åœºæ™¯
        test_scenes = available_scenes[:num_scenes]
        print(f"   ğŸ¯ æµ‹è¯•åœºæ™¯: {test_scenes}")
        
        # æ‰¹é‡å¤„ç†
        start_time = time.time()
        results = {}
        execution_times = {}
        
        for i, scene in enumerate(test_scenes, 1):
            print(f"\n   [{i}/{len(test_scenes)}] å¤„ç†åœºæ™¯: {scene}")
            
            # æ‰¹é‡å¤„ç†æ—¶å¯ä»¥é€‰æ‹©æ€§ç¦ç”¨å•ä¸ªå¯è§†åŒ–
            scene_visualization = enable_visualization and not create_batch_summary
            
            result = test_single_scene(
                scene_name=scene,
                enable_visualization=scene_visualization,
                device=device,
                use_vggt=use_vggt
            )
            
            if result and result['success']:
                results[scene] = result['pose']
                execution_times[scene] = result['execution_time']
            else:
                results[scene] = None
                execution_times[scene] = 0
        
        total_elapsed = time.time() - start_time
        
        # åˆ›å»ºæ‰¹é‡æ€»ç»“
        if create_batch_summary and enable_visualization:
            try:
                from camera_search import create_visualization_summary
                summary_path = create_visualization_summary(results, execution_times)
                print(f"   ğŸ“‹ æ‰¹é‡æ€»ç»“: {Path(summary_path).name}")
            except Exception as e:
                print(f"   âš ï¸ æ‰¹é‡æ€»ç»“ç”Ÿæˆå¤±è´¥: {e}")
        
        # ç»Ÿè®¡ç»“æœ
        successful = sum(1 for result in results.values() if result is not None)
        success_rate = (successful / len(test_scenes)) * 100
        avg_time = total_elapsed / len(test_scenes)
        
        print(f"\n   âœ… æ‰¹é‡å¤„ç†å®Œæˆ!")
        print(f"   âœ… æˆåŠŸç‡: {successful}/{len(test_scenes)} ({success_rate:.1f}%)")
        print(f"   â±ï¸ æ€»è€—æ—¶: {total_elapsed:.1f}ç§’")
        print(f"   â±ï¸ å¹³å‡è€—æ—¶: {avg_time:.1f}ç§’/åœºæ™¯")
        
        # æ˜¾ç¤ºç»“æœæ‘˜è¦
        for scene, pose in results.items():
            if pose is not None:
                print(f"   {scene}: ä»°è§’={pose.elevation:.1f}Â°, æ–¹ä½è§’={pose.azimuth:.1f}Â°")
            else:
                print(f"   {scene}: å¤±è´¥")
        
        return {
            'results': results,
            'execution_times': execution_times,
            'success_rate': success_rate,
            'total_time': total_elapsed,
            'average_time': avg_time
        }
        
    except Exception as e:
        print(f"   âŒ æ‰¹é‡æµ‹è¯•å¤±è´¥: {e}")
        return {'success_rate': 0, 'error': str(e)}

def main():
    """ä¸»å‡½æ•° - æ”¯æŒå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='V2M4ç®—æ³•ç»Ÿä¸€æµ‹è¯•è„šæœ¬')
    
    # åŸºç¡€å‚æ•°
    parser.add_argument('--scenes', '-n', type=int, default=3, 
                       help='æµ‹è¯•åœºæ™¯æ•°é‡ (é»˜è®¤: 3)')
    parser.add_argument('--cuda-device', type=int, default=2,
                       help='CUDAè®¾å¤‡ID (é»˜è®¤: 2)')
    parser.add_argument('--device', choices=['cuda', 'cpu'], default='cuda',
                       help='è®¡ç®—è®¾å¤‡ (é»˜è®¤: cuda)')
    
    # åŠŸèƒ½æ§åˆ¶
    parser.add_argument('--no-visualization', action='store_true',
                       help='ç¦ç”¨å¯è§†åŒ–åŠŸèƒ½')
    parser.add_argument('--no-batch-summary', action='store_true',
                       help='ç¦ç”¨æ‰¹é‡æ€»ç»“')
    parser.add_argument('--test-components', action='store_true',
                       help='æµ‹è¯•å¯è§†åŒ–ç»„ä»¶ (ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®)')
    parser.add_argument('--single-scene', type=str,
                       help='åªæµ‹è¯•æŒ‡å®šåœºæ™¯')
    parser.add_argument('--use-vggt', action='store_true',
                       help='ä½¿ç”¨VGGTæ¨¡å‹è¿›è¡Œæµ‹è¯•')
    
    args = parser.parse_args()
    
    # è®¾ç½®CUDAè®¾å¤‡
    if args.device == 'cuda':
        set_cuda_device(args.cuda_device)
    
    # æ ‡é¢˜
    print("ğŸš€ V2M4ç®—æ³•ç»Ÿä¸€æµ‹è¯•")
    print("=" * 50)
    
    # ç¯å¢ƒæ£€æŸ¥
    if not test_environment():
        print("âŒ ç¯å¢ƒæ£€æŸ¥å¤±è´¥ï¼Œé€€å‡ºæµ‹è¯•")
        return
    
    # è¿è¡Œæµ‹è¯•
    test_results = []
    
    # æµ‹è¯•1: å¯è§†åŒ–ç»„ä»¶æµ‹è¯• (å¯é€‰)
    if args.test_components:
        test_results.append(test_visualization_components())
    
    # æµ‹è¯•2: å•åœºæ™¯æµ‹è¯•
    if args.single_scene:
        print(f"\nğŸ¯ å•åœºæ™¯æµ‹è¯•æ¨¡å¼: {args.single_scene}")
        result = test_single_scene(
            scene_name=args.single_scene,
            enable_visualization=not args.no_visualization,
            device=args.device,
            use_vggt=args.use_vggt
        )
        test_results.append(result['success'] if result else False)
    else:
        # æµ‹è¯•3: å¤šåœºæ™¯æ‰¹é‡æµ‹è¯•
        batch_result = test_multiple_scenes(
            num_scenes=args.scenes,
            enable_visualization=not args.no_visualization,
            create_batch_summary=not args.no_batch_summary,
            device=args.device,
            use_vggt=args.use_vggt
        )
        test_results.append(batch_result['success_rate'] > 0)
    
    # æ€»ç»“
    print("\n" + "=" * 50)
    passed_tests = sum(test_results)
    total_tests = len(test_results)
    
    if passed_tests == total_tests:
        print(f"ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡! ({passed_tests}/{total_tests})")
        print("ğŸ“Š V2M4ç®—æ³•è¿è¡Œæ­£å¸¸!")
        
        # æ˜¾ç¤ºè¾“å‡ºç›®å½•
        output_dir = Path("outputs/visualization")
        if output_dir.exists():
            viz_files = list(output_dir.glob("*.png"))
            if viz_files:
                print(f"\nğŸ“ å¯è§†åŒ–æ–‡ä»¶: {len(viz_files)}ä¸ª")
                print(f"   ä½ç½®: {output_dir}")
        
        print(f"\nğŸ’¡ ä½¿ç”¨ç¤ºä¾‹:")
        print(f"   python test.py --scenes 5                    # æµ‹è¯•5ä¸ªåœºæ™¯")
        print(f"   python test.py --single-scene 'dancing_spiderman'  # æµ‹è¯•å•ä¸ªåœºæ™¯")
        print(f"   python test.py --single-scene 'dancing_spiderman' --use-vggt  # ä½¿ç”¨VGGTæµ‹è¯•")
        print(f"   python test.py --no-visualization            # ç¦ç”¨å¯è§†åŒ–")
        print(f"   python test.py --scenes 25                   # æµ‹è¯•æ‰€æœ‰åœºæ™¯")
        print(f"   python test.py --scenes 5 --use-vggt         # ä½¿ç”¨VGGTæ‰¹é‡æµ‹è¯•")
        
    else:
        print(f"âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥: {passed_tests}/{total_tests}")
        print("è¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯å¹¶ä¿®å¤é—®é¢˜")

if __name__ == "__main__":
    main()
