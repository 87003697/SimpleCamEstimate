"""
æ ¸å¿ƒV2M4ç›¸æœºæœç´¢ç®—æ³•å®ç°
åŒ…å«æ•°æ®ç»“æ„ã€å‡ ä½•å·¥å…·å’Œä¸»ç®—æ³•ç±»
"""

import os
import sys
import tempfile
import atexit
from dataclasses import dataclass
from typing import List, Optional, Tuple, Dict, Any
import torch
import cv2
import trimesh
import numpy as np
from pathlib import Path
from scipy.spatial.distance import cdist
import random

@dataclass
class CameraPose:
    """ç®€åŒ–çš„ç›¸æœºå§¿æ€å‚æ•° - ä¸KiuiKitå…¼å®¹"""
    elevation: float    # ä»°è§’ (åº¦)ï¼Œæ­£å€¼è¡¨ç¤ºç›¸æœºåœ¨ç‰©ä½“ä¸Šæ–¹  
    azimuth: float      # æ–¹ä½è§’ (åº¦)ï¼Œç»•å‚ç›´è½´æ—‹è½¬
    radius: float       # ç›¸æœºåˆ°ç›®æ ‡ç‚¹çš„è·ç¦»
    center_x: float = 0.0    # ç›®æ ‡ç‚¹xåæ ‡ (ç›¸æœºè§‚å¯Ÿçš„ä¸­å¿ƒç‚¹)
    center_y: float = 0.0    # ç›®æ ‡ç‚¹yåæ ‡
    center_z: float = 0.0    # ç›®æ ‡ç‚¹zåæ ‡
    
    @property
    def target_point(self) -> Tuple[float, float, float]:
        """è·å–ç›®æ ‡ç‚¹åæ ‡ï¼Œä¸kiui_mesh_renderer.pyå…¼å®¹"""
        return (self.center_x, self.center_y, self.center_z)
    
    def apply_to_kiui_camera(self, camera) -> None:
        """ç›´æ¥åº”ç”¨åˆ°KiuiKit OrbitCamera"""
        camera.from_angle(elevation=self.elevation, azimuth=self.azimuth, is_degree=True)
        camera.radius = self.radius
        camera.center = torch.tensor([self.center_x, self.center_y, self.center_z], dtype=torch.float32)
    
    def get_kiui_render_params(self) -> Dict[str, Any]:
        """è·å–kiui_mesh_renderer.render_single_view()çš„å‚æ•°"""
        return {
            'elevation': self.elevation,
            'azimuth': self.azimuth, 
            'distance': self.radius,
            'target_point': self.target_point
        }
    
    def to_matrix(self) -> torch.Tensor:
        """è½¬æ¢ä¸º4x4å˜æ¢çŸ©é˜µ"""
        # çƒåæ ‡åˆ°ç¬›å¡å°”åæ ‡
        elev_rad = torch.deg2rad(torch.tensor(self.elevation))
        azim_rad = torch.deg2rad(torch.tensor(self.azimuth))
        
        x = self.radius * torch.cos(elev_rad) * torch.cos(azim_rad)
        y = self.radius * torch.cos(elev_rad) * torch.sin(azim_rad)
        z = self.radius * torch.sin(elev_rad)
        
        # æ„é€ è§†å›¾çŸ©é˜µ
        camera_pos = torch.tensor([x, y, z], dtype=torch.float32)
        target = torch.tensor([self.center_x, self.center_y, self.center_z], dtype=torch.float32)
        up = torch.tensor([0, 0, 1], dtype=torch.float32)
        
        # Look-atçŸ©é˜µ
        forward = target - camera_pos
        forward = forward / (torch.linalg.norm(forward) + 1e-8)
        
        right = torch.linalg.cross(forward, up)
        right = right / (torch.linalg.norm(right) + 1e-8)
        
        up = torch.linalg.cross(right, forward)
        
        view_matrix = torch.eye(4, dtype=torch.float32)
        view_matrix[:3, 0] = right
        view_matrix[:3, 1] = up
        view_matrix[:3, 2] = -forward
        view_matrix[:3, 3] = camera_pos
        
        return view_matrix
    
    def __str__(self) -> str:
        return f"CameraPose(elev={self.elevation:.1f}Â°, azim={self.azimuth:.1f}Â°, r={self.radius:.2f}, center=({self.center_x:.2f}, {self.center_y:.2f}, {self.center_z:.2f}))"

@dataclass
class DataPair:
    """æ•°æ®å¯¹ç»“æ„ - é€‚é…ç®€åŒ–çš„æ•°æ®æ ¼å¼"""
    scene_name: str              # åœºæ™¯åç§°
    mesh_path: str              # Meshæ–‡ä»¶è·¯å¾„: data/meshes/{scene_name}_textured_frame_000000.glb
    image_path: str             # å›¾åƒæ–‡ä»¶è·¯å¾„: data/images/{scene_name}.png
    
    @classmethod
    def from_scene_name(cls, scene_name: str, data_dir: str = "data"):
        """æ ¹æ®åœºæ™¯åç§°åˆ›å»ºæ•°æ®å¯¹"""
        mesh_path = f"{data_dir}/meshes/{scene_name}_textured_frame_000000.glb"
        image_path = f"{data_dir}/images/{scene_name}.png"
        return cls(scene_name, mesh_path, image_path)
    
    def exists(self) -> bool:
        """æ£€æŸ¥æ•°æ®å¯¹æ˜¯å¦å­˜åœ¨"""
        return Path(self.mesh_path).exists() and Path(self.image_path).exists()
    
    def load_mesh(self) -> trimesh.Trimesh:
        """åŠ è½½meshå¯¹è±¡"""
        if not Path(self.mesh_path).exists():
            raise FileNotFoundError(f"Mesh file not found: {self.mesh_path}")
        
        loaded = trimesh.load(self.mesh_path)
        
        # å¤„ç†GLBæ–‡ä»¶å¯èƒ½è¿”å›Sceneå¯¹è±¡çš„æƒ…å†µ
        if hasattr(loaded, 'geometry') and loaded.geometry:
            # Sceneå¯¹è±¡ï¼Œæå–ç¬¬ä¸€ä¸ªå‡ ä½•ä½“
            mesh_name = list(loaded.geometry.keys())[0]
            return loaded.geometry[mesh_name]
        elif hasattr(loaded, 'vertices'):
            # ç›´æ¥æ˜¯Meshå¯¹è±¡
            return loaded
        else:
            raise ValueError(f"Could not extract mesh from file: {self.mesh_path}")

class DataManager:
    """æ•°æ®å‘ç°å’ŒéªŒè¯ç®¡ç†å™¨"""
    
    def __init__(self, data_dir: str = "data"):
        self.data_dir = Path(data_dir)
        self.meshes_dir = self.data_dir / "meshes"
        self.images_dir = self.data_dir / "images"
    
    def discover_data_pairs(self) -> List[DataPair]:
        """å‘ç°æ‰€æœ‰å¯ç”¨çš„æ•°æ®å¯¹"""
        data_pairs = []
        
        if not self.meshes_dir.exists() or not self.images_dir.exists():
            return data_pairs
        
        # ä»meshæ–‡ä»¶æ¨æ–­åœºæ™¯åç§°
        for mesh_file in self.meshes_dir.glob("*_textured_frame_000000.glb"):
            scene_name = mesh_file.stem.replace("_textured_frame_000000", "")
            data_pair = DataPair.from_scene_name(scene_name, str(self.data_dir))
            
            if data_pair.exists():
                data_pairs.append(data_pair)
        
        return data_pairs
    
    def validate_data_structure(self) -> Dict:
        """éªŒè¯æ•°æ®ç»“æ„"""
        validation = {
            'total_mesh_files': 0,
            'total_image_files': 0,
            'valid_data_pairs': 0,
            'data_completeness': 0.0,
            'missing_scenes': []
        }
        
        if self.meshes_dir.exists():
            validation['total_mesh_files'] = len(list(self.meshes_dir.glob("*.glb")))
        
        if self.images_dir.exists():
            validation['total_image_files'] = len(list(self.images_dir.glob("*.png")))
        
        data_pairs = self.discover_data_pairs()
        validation['valid_data_pairs'] = len(data_pairs)
        
        if validation['total_mesh_files'] > 0:
            validation['data_completeness'] = (validation['valid_data_pairs'] / validation['total_mesh_files']) * 100
        
        return validation

class GeometryUtils:
    """å‡ ä½•å·¥å…· - åªä¿ç•™æ ¸å¿ƒåŠŸèƒ½"""
    
    @staticmethod
    def sample_sphere_poses(num_samples: int) -> List[CameraPose]:
        """çƒé¢ç­‰é¢ç§¯é‡‡æ ·"""
        poses = []
        for _ in range(num_samples):
            # ç­‰é¢ç§¯é‡‡æ ·
            u, v = torch.rand(2)
            elevation = torch.rad2deg(torch.asin(2 * u - 1))  # [-90Â°, 90Â°]
            azimuth = 360 * v  # [0Â°, 360Â°]
            
            # è·ç¦»é‡‡æ · (å¹³æ–¹æ ¹åˆ†å¸ƒ)
            radius = 1.0 + 4.0 * torch.sqrt(torch.rand(1))
            
            # è½»å¾®çš„ä¸­å¿ƒç‚¹éšæœºåŒ–
            center_x = torch.rand(1) * 1.0 - 0.5  # [-0.5, 0.5]
            center_y = torch.rand(1) * 1.0 - 0.5
            center_z = torch.rand(1) * 1.0 - 0.5
            
            poses.append(CameraPose(
                elevation=elevation.item(),
                azimuth=azimuth.item(),
                radius=radius.item(),
                center_x=center_x.item(),
                center_y=center_y.item(),
                center_z=center_z.item()
            ))
        
        return poses
    
    @staticmethod
    def align_pointclouds_simple(reference_pc: torch.Tensor, 
                               rendered_pcs: List[torch.Tensor],
                               poses: List[CameraPose]) -> Optional[CameraPose]:
        """ç®€åŒ–çš„ç‚¹äº‘å¯¹é½æ–¹æ³•"""
        if len(rendered_pcs) == 0:
            return None
        
        reference_pc = GeometryUtils._clean_pointcloud(reference_pc)
        if reference_pc is None:
            return None
        
        best_pose = None
        best_score = float('inf')
        
        for pc, pose in zip(rendered_pcs, poses):
            pc = GeometryUtils._clean_pointcloud(pc)
            if pc is None:
                continue
            
            score = GeometryUtils._chamfer_distance(reference_pc, pc)
            if score < best_score:
                best_score = score
                best_pose = pose
        
        return best_pose
    
    @staticmethod
    def _clean_pointcloud(pc: torch.Tensor) -> Optional[torch.Tensor]:
        """æ¸…ç†ç‚¹äº‘æ•°æ®ï¼Œç§»é™¤æ— æ•ˆç‚¹"""
        if pc is None:
            return None
        
        # ç»Ÿä¸€è½¬æ¢ä¸ºtensor
        if not isinstance(pc, torch.Tensor):
            if hasattr(pc, 'shape') and hasattr(pc, 'astype'):
                # numpy array
                pc = torch.from_numpy(pc.astype('float32'))
            else:
                return None
        
        # æ£€æŸ¥å½¢çŠ¶
        if pc.dim() != 2 or pc.shape[1] != 3:
            return None
        
        # æ£€æŸ¥å¤§å°
        if pc.shape[0] < 10:
            return None
        
        # ç§»é™¤æ— æ•ˆå€¼
        valid_mask = torch.isfinite(pc).all(dim=1)
        pc = pc[valid_mask]
        
        if pc.shape[0] < 10:
            return None
        
        return pc
    
    @staticmethod
    def _chamfer_distance(pc1: torch.Tensor, pc2: torch.Tensor) -> float:
        """è®¡ç®—å€’è§’è·ç¦»"""
        if pc1.shape[0] == 0 or pc2.shape[0] == 0:
            return float('inf')
        
        # ç¡®ä¿åœ¨åŒä¸€è®¾å¤‡ä¸Š
        device = pc1.device
        pc2 = pc2.to(device)
        
        # é‡‡æ ·ä»¥æé«˜æ•ˆç‡
        if pc1.shape[0] > 1000:
            indices = torch.randperm(pc1.shape[0])[:1000]
            pc1 = pc1[indices]
        
        if pc2.shape[0] > 1000:
            indices = torch.randperm(pc2.shape[0])[:1000]
            pc2 = pc2[indices]
        
        # è®¡ç®—è·ç¦»çŸ©é˜µ
        # pc1: (N, 3), pc2: (M, 3)
        # dist_matrix: (N, M)
        dist_matrix = torch.cdist(pc1, pc2)
        
        # å€’è§’è·ç¦»
        forward = torch.mean(torch.min(dist_matrix, dim=1)[0])
        backward = torch.mean(torch.min(dist_matrix, dim=0)[0])
        
        return (forward + backward).item() / 2.0

class MeshRenderer:
    """Meshæ¸²æŸ“å™¨ - åŸºäºStandardKiuiRenderer"""
    
    def __init__(self, device: str = "cuda"):
        self.device = device
        self._renderer = None
        self._cached_mesh_path = None
        self._mesh_loaded = False
        
        # æ£€æŸ¥ä¾èµ–
        self._check_dependencies()
    
    def _check_dependencies(self):
        """æ£€æŸ¥å¿…è¦çš„ä¾èµ–"""
        # æ£€æŸ¥KiuiKit
        from kiui.mesh import Mesh as KiuiMesh
        from kiui.cam import OrbitCamera
        self.kiui_available = True
        
        # æ£€æŸ¥nvdiffrast
        import nvdiffrast.torch as dr
        self.nvdiffrast_available = True
        
        # æ£€æŸ¥StandardKiuiRenderer
        sys.path.append(str(Path(__file__).parent.parent / "_reference" / "MeshSeriesGen" / "tools" / "visualization"))
        from kiui_mesh_renderer import StandardKiuiRenderer
        self.renderer_available = True
    
    @property
    def renderer(self):
        """å»¶è¿Ÿåˆå§‹åŒ–æ¸²æŸ“å™¨"""
        if self._renderer is None:
            from kiui_mesh_renderer import StandardKiuiRenderer
            self._renderer = StandardKiuiRenderer(
                width=512, 
                height=512, 
                background_color=(1.0, 1.0, 1.0),
                device=self.device
            )
        return self._renderer
    
    def prepare_mesh(self, mesh: trimesh.Trimesh) -> str:
        """å‡†å¤‡meshç”¨äºæ¸²æŸ“ - å¯¼å‡ºä¸ºä¸´æ—¶æ–‡ä»¶"""
        if self._cached_mesh_path is None:
            # åˆ›å»ºæŒä¹…çš„ä¸´æ—¶æ–‡ä»¶
            temp_fd, temp_path = tempfile.mkstemp(suffix='.obj', prefix='v2m4_mesh_')
            os.close(temp_fd)
            
            # å¯¼å‡ºmesh
            mesh.export(temp_path)
            self._cached_mesh_path = temp_path
            
            # æ³¨å†Œæ¸…ç†å‡½æ•°
            def cleanup_cached_mesh():
                if self._cached_mesh_path and os.path.exists(self._cached_mesh_path):
                    os.unlink(self._cached_mesh_path)
            atexit.register(cleanup_cached_mesh)
        
        return self._cached_mesh_path
    
    def load_mesh_to_renderer(self, mesh_path: str):
        """åŠ è½½meshåˆ°æ¸²æŸ“å™¨"""
        if not self._mesh_loaded or self.renderer.mesh_path_loaded != mesh_path:
            loaded_mesh = self.renderer.load_mesh(mesh_path)
            if loaded_mesh is None:
                raise RuntimeError(f"Could not load mesh to renderer: {mesh_path}")
            self._mesh_loaded = True
    
    def render_single_view(self, mesh: trimesh.Trimesh, pose: CameraPose) -> torch.Tensor:
        """æ¸²æŸ“å•ä¸ªè§†å›¾"""
        # å‡†å¤‡mesh
        mesh_path = self.prepare_mesh(mesh)
        self.load_mesh_to_renderer(mesh_path)
        
        # æ¸²æŸ“
        rendered_img = self.renderer.render_single_view(
            elevation=pose.elevation,
            azimuth=pose.azimuth,
            distance=pose.radius,
            target_point=pose.target_point,
            render_mode='lambertian'
        )
        
        if rendered_img is None:
            raise RuntimeError(f"Rendering failed: {pose}")
        
        return rendered_img
    
    def render_batch_views(self, mesh: trimesh.Trimesh, poses: List[CameraPose], 
                          max_batch_size: int = 8) -> List[torch.Tensor]:
        """æ‰¹é‡æ¸²æŸ“å¤šä¸ªè§†å›¾"""
        # å‡†å¤‡mesh
        mesh_path = self.prepare_mesh(mesh)
        self.load_mesh_to_renderer(mesh_path)
        
        # å‡†å¤‡æ‰¹é‡æ¸²æŸ“å‚æ•°
        camera_params = []
        for pose in poses:
            camera_params.append({
                'elevation': pose.elevation,
                'azimuth': pose.azimuth,
                'distance': pose.radius,
                'target_point': pose.target_point
            })
        
        # æ‰§è¡Œæ‰¹é‡æ¸²æŸ“
        rendered_images = self.renderer.render_batch_views(
            camera_params=camera_params,
            render_mode='lambertian',
            max_batch_size=max_batch_size
        )
        
        # æ£€æŸ¥ç»“æœ
        valid_images = []
        for i, img in enumerate(rendered_images):
            if img is None:
                raise RuntimeError(f"Batch rendering failed: pose {i}")
            valid_images.append(img)
        
        return valid_images

class CleanV2M4CameraSearch:
    """ç®€åŒ–çš„V2M4ç›¸æœºæœç´¢ç®—æ³• - æ ¸å¿ƒå®ç°"""
    
    def __init__(self, dust3r_model_path: str, device: str = "cuda", enable_visualization: bool = True):
        self.dust3r_model_path = dust3r_model_path
        self.device = device
        self.enable_visualization = enable_visualization
        
        # ä¼˜åŒ–åçš„é…ç½®å‚æ•° - åŸºäºæ€§èƒ½æµ‹è¯•æœ€ä¼˜å€¼
        self.config = {
            'initial_samples': 512,       # åˆå§‹é‡‡æ ·æ•° (æµ·é€‰é˜¶æ®µ) - æå‡: 128â†’512
            'top_n': 7,                   # å€™é€‰æ•° (å‡ ä½•è§£å¯†)
            'pso_particles': 80,          # PSOç²’å­æ•° (å…¨å±€ä¼˜åŒ–) - ä¼˜åŒ–: 50â†’80
            'pso_iterations': 20,         # PSOè¿­ä»£æ•° (å…¨å±€ä¼˜åŒ–)
            'grad_iterations': 200,       # æ¢¯åº¦ä¸‹é™è¿­ä»£æ•° (ç²¾ç»†è°ƒæ•´) - ä¼˜åŒ–: 100â†’200
            'image_size': 512,            # å›¾åƒå°ºå¯¸
            'render_batch_size': 32,      # æ‰¹é‡æ¸²æŸ“å¤§å° - æå‡: 16â†’32 (å¹³è¡¡æ€§èƒ½å’Œå†…å­˜)
            'max_batch_size': 8,          # æ¸²æŸ“å™¨æœ€å¤§æ‰¹é‡å¤§å° (ç”¨äºTop-Né€‰æ‹©å’ŒPSOæœç´¢)
            
            # æ–°å¢çš„ä¼˜åŒ–å‚æ•°
            'dust3r_alignment_iterations': 1000,  # DUSt3Rå¯¹é½è¿­ä»£æ•° - ä¼˜åŒ–: 300â†’1000
            'pso_w': 0.6,                        # PSOæƒ¯æ€§æƒé‡ - ä¼˜åŒ–: 0.7â†’0.6
            'pso_c1': 1.0,                       # PSOä¸ªä½“å­¦ä¹ å› å­ - ä¼˜åŒ–: 1.5â†’1.0
            'top_k_for_pso': 100,                # PSOé€‰æ‹©çš„top-kå€™é€‰
            'point_cloud_sample_ratio': 0.05,    # ç‚¹äº‘é‡‡æ ·æ¯”ä¾‹
            'min_confidence': 0.3,               # æœ€å°ç½®ä¿¡åº¦é˜ˆå€¼
            
            # æ¨¡å‹é€‰æ‹©é…ç½®
            'use_dust3r': True,                  # æ˜¯å¦ä½¿ç”¨DUSt3Ræ¨¡å‹
            'model_name': 'dust3r',              # æ¨¡å‹åç§°æ ‡è¯†
            'skip_model_step': False,            # æ˜¯å¦è·³è¿‡æ¨¡å‹ä¼°è®¡æ­¥éª¤
            
            # æ€§èƒ½ä¼˜åŒ–é…ç½®
            'use_batch_optimization': True       # æ˜¯å¦ä½¿ç”¨æ‰¹é‡ä¼˜åŒ–ï¼ˆPSOå’Œæ¢¯åº¦ä¸‹é™ï¼‰
        }
        
        # å»¶è¿Ÿåˆå§‹åŒ–ç»„ä»¶
        self._dust3r_helper = None
        self._renderer = None
        self._optimizer = None
        self._visualizer = None
        
        # æ ¹æ®é…ç½®æ›´æ–°æ¨¡å‹åç§°
        if self.config['use_dust3r']:
            self.config['model_name'] = 'dust3r'
        else:
            self.config['model_name'] = 'dust3r' # ç¡®ä¿é»˜è®¤ä½¿ç”¨dust3r
        
        # å¯è§†åŒ–æ•°æ®æ”¶é›†
        self.visualization_data = {
            'progression': [],
            'mesh_info': {},
            'algorithm_stats': {}
        }
    
    @property
    def dust3r_helper(self):
        """å»¶è¿Ÿåˆå§‹åŒ–DUSt3RåŠ©æ‰‹"""
        if self._dust3r_helper is None:
            from .dust3r_helper import DUSt3RHelper
            self._dust3r_helper = DUSt3RHelper(self.dust3r_model_path, self.device)
        return self._dust3r_helper

    @property
    def renderer(self):
        """å»¶è¿Ÿåˆå§‹åŒ–æ¸²æŸ“å™¨"""
        if self._renderer is None:
            self._renderer = MeshRenderer(self.device)
        return self._renderer
    
    @property
    def optimizer(self):
        """ä¼˜åŒ–å™¨ - æ‡’åŠ è½½"""
        if self._optimizer is None:
            from .optimizer import OptimizerManager
            
            # ä½¿ç”¨ä¼˜åŒ–åçš„PSOå‚æ•°
            pso_params = {
                'num_particles': self.config['pso_particles'],
                'max_iterations': self.config['pso_iterations'], 
                'w': 0.6,      # ä¼˜åŒ–ï¼šæƒ¯æ€§æƒé‡ 0.7â†’0.6
                'c1': 1.0,     # ä¼˜åŒ–ï¼šä¸ªä½“å­¦ä¹ å› å­ 1.5â†’1.0
                'c2': 1.5      # ä¿æŒï¼šç¤¾ä¼šå­¦ä¹ å› å­
            }
            
            gd_params = {
                'learning_rate': 0.01,
                'max_iterations': self.config['grad_iterations'],
                'tolerance': 1e-6
            }
            
            self._optimizer = OptimizerManager(pso_params=pso_params, gd_params=gd_params)
        
        return self._optimizer
    
    @property
    def visualizer(self):
        """å»¶è¿Ÿåˆå§‹åŒ–å¯è§†åŒ–å™¨"""
        if self._visualizer is None and self.enable_visualization:
            from .visualization import V2M4Visualizer
            self._visualizer = V2M4Visualizer(output_dir="outputs/visualization")
        return self._visualizer
    
    def search_camera_pose(self, data_pair: DataPair, save_visualization: bool = True) -> CameraPose:
        """ä¸»ç®—æ³•å…¥å£ - V2M4çš„9ä¸ªæ ¸å¿ƒæ­¥éª¤"""
        
        import time
        start_time = time.time()
        
        print(f"ğŸ¬ Starting V2M4 camera search algorithm...")
        print(f"   Scene: {data_pair.scene_name}")
        print(f"   Mesh: {data_pair.mesh_path}")
        print(f"   Image: {data_pair.image_path}")
        
        # éªŒè¯æ•°æ®å­˜åœ¨
        if not data_pair.exists():
            raise FileNotFoundError(f"Data pair incomplete: {data_pair.scene_name}")
        
        # åŠ è½½æ•°æ®
        reference_image = self._load_image(data_pair.image_path)
        mesh = data_pair.load_mesh()
        
        # æ”¶é›†meshä¿¡æ¯ç”¨äºå¯è§†åŒ–
        if self.enable_visualization:
            bounds = mesh.bounds
            self.visualization_data['mesh_info'] = {
                'vertices_count': len(mesh.vertices),
                'faces_count': len(mesh.faces),
                'bounds': bounds.tolist(),
                'center': mesh.centroid.tolist(),
                'scale': float(torch.linalg.norm(torch.from_numpy(bounds[1] - bounds[0]).float()))
            }
        
        # æ­¥éª¤1: é‡‡æ ·åˆå§‹ç›¸æœºpose
        print("ğŸ“ Step 1: Sphere sampling camera poses...")
        initial_poses = self._sample_sphere_poses()
        
        # æ­¥éª¤2: æ¸²æŸ“å¹¶é€‰æ‹©top-n
        print("ğŸ¯ Step 2: Selecting top-n candidate poses...")
        top_poses = self._select_top_poses(mesh, reference_image, initial_poses)
        
        # å¯è§†åŒ–ï¼šè®°å½•top-1å§¿æ€
        if self.enable_visualization and top_poses:
            rendered_top1 = self.renderer.render_single_view(mesh, top_poses[0])
            similarity_top1 = self._compute_similarity(reference_image, rendered_top1)
            self.visualization_data['progression'].append({
                'step_name': 'Top-1 Selection',
                'pose': top_poses[0],
                'rendered_image': rendered_top1,
                'similarity': similarity_top1,
                'score': similarity_top1
            })
        
        # æ­¥éª¤3-4: æ¨¡å‹ä¼°è®¡ (å‡ ä½•çº¦æŸ - DUSt3Ræˆ–VGGT)
        if self.config.get('skip_model_step', False):
            print(f"ğŸ”„ Step 3-4: Skipping model estimation step...")
        else:
            print(f"ğŸ” Step 3-4: {self.config['model_name'].upper()} geometric constraint estimation...")
        model_pose = self._model_estimation(mesh, reference_image, top_poses)
        
        # å¯è§†åŒ–ï¼šè®°å½•æ¨¡å‹ç»“æœ
        if self.enable_visualization and model_pose:
            rendered_model = self.renderer.render_single_view(mesh, model_pose)
            similarity_model = self._compute_similarity(reference_image, rendered_model)
            self.visualization_data['progression'].append({
                'step_name': f'{self.config["model_name"].upper()} Align',
                'pose': model_pose,
                'rendered_image': rendered_model,
                'similarity': similarity_model,
                'score': similarity_model
            })
        
        # æ­¥éª¤5-6: PSOæœç´¢
        print("ğŸ” Step 5-6: PSO particle swarm optimization...")
        pso_pose = self._pso_search(mesh, reference_image, model_pose, top_poses)
        
        # å¯è§†åŒ–ï¼šè®°å½•PSOç»“æœ
        if self.enable_visualization:
            rendered_pso = self.renderer.render_single_view(mesh, pso_pose)
            similarity_pso = self._compute_similarity(reference_image, rendered_pso)
            self.visualization_data['progression'].append({
                'step_name': 'PSO Optimize',
                'pose': pso_pose,
                'rendered_image': rendered_pso,
                'similarity': similarity_pso,
                'score': similarity_pso
            })
        
        # æ­¥éª¤7-8: æ¢¯åº¦ä¸‹é™ç²¾åŒ–
        print("ğŸ¯ Step 7-8: Gradient descent refinement...")
        final_pose = self._gradient_refinement(mesh, reference_image, pso_pose)
        
        # è®¡ç®—æ‰§è¡Œæ—¶é—´
        execution_time = time.time() - start_time
        
        # æ”¶é›†ç®—æ³•ç»Ÿè®¡ä¿¡æ¯
        if self.enable_visualization:
            final_rendered = self.renderer.render_single_view(mesh, final_pose)
            final_similarity = self._compute_similarity(reference_image, final_rendered)
            
            self.visualization_data['progression'].append({
                'step_name': 'Final Result',
                'pose': final_pose,
                'rendered_image': final_rendered,
                'similarity': final_similarity,
                'score': final_similarity
            })
            
            self.visualization_data['algorithm_stats'] = {
                'initial_samples': self.config['initial_samples'],
                'top_n': self.config['top_n'],
                'pso_iterations': self.config['pso_iterations'],
                'final_score': final_similarity
            }
        
        # ç”Ÿæˆå¯è§†åŒ–ç»“æœ
        if self.enable_visualization and save_visualization and self.visualizer:
            # åˆ›å»ºç»“æœå¯¹æ¯”å›¾
            final_rendered = self.renderer.render_single_view(mesh, final_pose)
            
            # è½¬æ¢tensorä¸ºnumpyæ•°ç»„ä¾›å¯è§†åŒ–ä½¿ç”¨
            ref_img_np = self._tensor_to_numpy(reference_image)
            rendered_img_np = self._tensor_to_numpy(final_rendered)
            
            comparison_path = self.visualizer.create_result_comparison(
                data_pair=data_pair,
                reference_image=ref_img_np,
                rendered_result=rendered_img_np,
                final_pose=final_pose,
                mesh_info=self.visualization_data['mesh_info'],
                algorithm_stats=self.visualization_data['algorithm_stats'],
                execution_time=execution_time
            )
            
            # åˆ›å»ºä¼˜åŒ–è¿‡ç¨‹å¯è§†åŒ–
            if self.visualization_data['progression']:
                # è½¬æ¢progressionæ•°æ®ä¸­çš„å›¾åƒ
                progression_data_np = []
                for step_data in self.visualization_data['progression']:
                    step_data_np = step_data.copy()
                    if 'rendered_image' in step_data_np and step_data_np['rendered_image'] is not None:
                        step_data_np['rendered_image'] = self._tensor_to_numpy(step_data_np['rendered_image'])
                    progression_data_np.append(step_data_np)
                
                progression_path = self.visualizer.create_pose_progression_visualization(
                    data_pair=data_pair,
                    reference_image=ref_img_np,
                    progression_data=progression_data_np,
                    final_pose=final_pose
                )
            
            # ä¿å­˜å•ç‹¬çš„ç»“æœå›¾åƒ
            individual_paths = self.visualizer.save_individual_results(
                data_pair=data_pair,
                reference_image=ref_img_np,
                rendered_result=rendered_img_np
            )
        
        print("âœ… V2M4 camera search completed!")
        print(f"â±ï¸ Total execution time: {execution_time:.2f} seconds")
        
        return final_pose
    
    def _load_image(self, image_path: str) -> torch.Tensor:
        """åŠ è½½å›¾åƒæ–‡ä»¶"""
        image = cv2.imread(image_path)
        if image is None:
            raise ValueError(f"Could not load image: {image_path}")
        # è½¬æ¢ä¸ºRGBå¹¶ç¡®ä¿æ•°æ®ç±»å‹ä¸ºfloat32ï¼ŒèŒƒå›´[0,255]
        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        return torch.from_numpy(rgb_image.astype(np.float32))
    
    def _sample_sphere_poses(self) -> List[CameraPose]:
        """æ­¥éª¤1: çƒé¢ç­‰é¢ç§¯é‡‡æ ·"""
        return GeometryUtils.sample_sphere_poses(self.config['initial_samples'])
    
    def _select_top_poses(self, mesh: trimesh.Trimesh, reference_image: torch.Tensor, 
                         poses: List[CameraPose]) -> List[CameraPose]:
        """æ­¥éª¤2: åŸºäºç›¸ä¼¼åº¦é€‰æ‹©top-n - ä¼˜åŒ–æ‰¹é‡æ¸²æŸ“ä»¥é¿å…nvdiffrastå¡ä½"""
        
        print(f"   Evaluating {len(poses)} candidate poses...")
        print(f"   Using max batch size: {self.config.get('max_batch_size', 8)}")
        
        scores = []
        batch_size = self.config['render_batch_size']  # ä½¿ç”¨é…ç½®çš„æ‰¹é‡æ¸²æŸ“å¤§å°
        max_batch_size = self.config.get('max_batch_size', 8)  # è·å–æœ€å¤§æ‰¹é‡å¤§å°
        
        # ä½¿ç”¨no_gradä¼˜åŒ–æ€§èƒ½ï¼Œå› ä¸ºtop-né€‰æ‹©ä¸éœ€è¦æ¢¯åº¦
        with torch.no_grad():
            # å°è¯•æ‰¹é‡æ¸²æŸ“
            for i in range(0, len(poses), batch_size):
                batch_poses = poses[i:i+batch_size]
                print(f"   Batch rendering {i+1}-{min(i+batch_size, len(poses))}/{len(poses)}...")
                
                # æ‰¹é‡æ¸²æŸ“è¿™ä¸€ç»„ï¼Œä½¿ç”¨é…ç½®çš„max_batch_size
                rendered_images = self.renderer.render_batch_views(mesh, batch_poses, max_batch_size)
                
                # è®¡ç®—ç›¸ä¼¼åº¦åˆ†æ•°
                for rendered_img in rendered_images:
                    if rendered_img is not None:
                        score = self._compute_similarity(reference_image, rendered_img)
                        scores.append(score)
                    else:
                        scores.append(float('inf'))  # æ¸²æŸ“å¤±è´¥ï¼Œç»™æœ€å·®åˆ†æ•°
                
                # æ¸…ç†GPUå†…å­˜
                from .utils import cleanup_gpu_memory
                cleanup_gpu_memory()
        
        # é€‰æ‹©top-n
        if len(scores) != len(poses):
            print(f"   âš ï¸ Score count ({len(scores)}) does not match pose count ({len(poses)})")
            scores = scores[:len(poses)] + [float('inf')] * (len(poses) - len(scores))
        
        top_indices = torch.argsort(torch.tensor(scores))[:self.config['top_n']].tolist()
        selected_poses = [poses[i] for i in top_indices]
        
        print(f"   âœ… Selected {len(selected_poses)} best poses")
        print(f"   Best score: {min(scores):.4f}")
        
        return selected_poses
    
    def _model_estimation(self, mesh: trimesh.Trimesh, reference_image: torch.Tensor, 
                         top_poses: List[CameraPose]) -> Optional[CameraPose]:
        """æ­¥éª¤3-4: æ¨¡å‹ä¼°è®¡ - æ ¸å¿ƒå‡ ä½•çº¦æŸ (DUSt3Ræˆ–VGGT)"""
        
        # æ£€æŸ¥æ˜¯å¦è·³è¿‡æ¨¡å‹ä¼°è®¡æ­¥éª¤
        if self.config.get('skip_model_step', False):
            print("   ğŸ”„ Skipping model estimation step (as configured)")
            # è¿”å›æœ€ä½³çš„top poseä½œä¸ºæ¨¡å‹ä¼°è®¡ç»“æœ
            return top_poses[0] if top_poses else None
        
        # 1. æ¸²æŸ“top poses
        rendered_views = [self.renderer.render_single_view(mesh, pose) for pose in top_poses]
        
        # 2. æ ¹æ®é…ç½®é€‰æ‹©æ¨¡å‹è¿›è¡Œæ¨ç†
        if self.config['use_dust3r']:
            # ä½¿ç”¨DUSt3Ræ¨¡å‹
            model_result = self.dust3r_helper.inference(reference_image, rendered_views)
        else:
            # ä½¿ç”¨DUSt3Ræ¨¡å‹ (é»˜è®¤)
            model_result = self.dust3r_helper.inference(reference_image, rendered_views)
        
        # 3. ç‚¹äº‘å¯¹é½ (ç®€åŒ–ç‰ˆ)
        best_pose = GeometryUtils.align_pointclouds_simple(
            model_result.reference_pc,
            model_result.rendered_pcs, 
            top_poses[:len(model_result.rendered_pcs)]
        )
        
        return best_pose
    
    def _pso_search(self, mesh: trimesh.Trimesh, reference_image: torch.Tensor,
                   model_pose: Optional[CameraPose], top_poses: List[CameraPose]) -> CameraPose:
        """æ­¥éª¤5-6: PSOæœç´¢ - æ”¯æŒæ‰¹é‡å’Œä¼ ç»Ÿä¼˜åŒ–"""
        
        # å‡†å¤‡åˆå§‹å€™é€‰
        candidates = top_poses[:self.config['pso_particles']]
        if model_pose is not None:
            candidates.append(model_pose)
        
        if not candidates:
            return CameraPose(elevation=0, azimuth=0, radius=2.5)
        
        # å‚æ•°è¾¹ç•Œ
        bounds = {
            'elevation': (-90, 90),
            'azimuth': (0, 360),
            'radius': (1.0, 5.0),
            'center_x': (-1.0, 1.0),
            'center_y': (-1.0, 1.0),
            'center_z': (-1.0, 1.0)
        }
        
        # æ ¹æ®é…ç½®é€‰æ‹©ä¼˜åŒ–æ–¹å¼
        if self.config.get('use_batch_optimization', True):
            # ä½¿ç”¨æ‰¹é‡ä¼˜åŒ–
            def batch_objective(poses: List[CameraPose]) -> List[float]:
                with torch.no_grad():  # PSOä¼˜åŒ–ä¸éœ€è¦æ¢¯åº¦ï¼Œæ·»åŠ no_gradæå‡æ€§èƒ½
                    max_batch_size = self.config.get('max_batch_size', 8)
                    rendered_images = self.renderer.render_batch_views(mesh, poses, max_batch_size)
                    scores = []
                    for rendered_img in rendered_images:
                        if rendered_img is not None:
                            score = self._compute_similarity(reference_image, rendered_img)
                            scores.append(score)
                        else:
                            scores.append(float('inf'))  # æ¸²æŸ“å¤±è´¥ï¼Œç»™æœ€å·®åˆ†æ•°
                    return scores
            
            return self.optimizer.pso_optimize_batch(batch_objective, candidates, bounds)
        else:
            # ä½¿ç”¨ä¼ ç»Ÿå•æ¬¡ä¼˜åŒ–
            def objective(pose: CameraPose) -> float:
                with torch.no_grad():  # PSOä¼˜åŒ–ä¸éœ€è¦æ¢¯åº¦ï¼Œæ·»åŠ no_gradæå‡æ€§èƒ½
                    rendered = self.renderer.render_single_view(mesh, pose)
                    return self._compute_similarity(reference_image, rendered)
            
            return self.optimizer.pso_optimize(objective, candidates, bounds)
    
    def _gradient_refinement(self, mesh: trimesh.Trimesh, reference_image: torch.Tensor,
                           initial_pose: CameraPose) -> CameraPose:
        """æ­¥éª¤7-8: æ¢¯åº¦ä¸‹é™ç²¾åŒ– - æ”¯æŒæ‰¹é‡å’Œä¼ ç»Ÿä¼˜åŒ–"""
        
        # æ ¹æ®é…ç½®é€‰æ‹©ä¼˜åŒ–æ–¹å¼
        if self.config.get('use_batch_optimization', True):
            # ä½¿ç”¨æ‰¹é‡ä¼˜åŒ–
            def batch_objective(poses: List[CameraPose]) -> List[float]:
                max_batch_size = self.config.get('max_batch_size', 8)
                rendered_images = self.renderer.render_batch_views(mesh, poses, max_batch_size)
                scores = []
                for rendered_img in rendered_images:
                    if rendered_img is not None:
                        score = self._compute_similarity(reference_image, rendered_img)
                        scores.append(score)
                    else:
                        scores.append(float('inf'))  # æ¸²æŸ“å¤±è´¥ï¼Œç»™æœ€å·®åˆ†æ•°
                return scores
            
            return self.optimizer.gradient_descent_batch(batch_objective, initial_pose)
        else:
            # ä½¿ç”¨ä¼ ç»Ÿå•æ¬¡ä¼˜åŒ–
            def objective(pose: CameraPose) -> float:
                rendered = self.renderer.render_single_view(mesh, pose)
                return self._compute_similarity(reference_image, rendered)
            
            return self.optimizer.gradient_descent(objective, initial_pose)
    
    def _compute_similarity(self, img1: torch.Tensor, img2) -> float:
        """è®¡ç®—å›¾åƒç›¸ä¼¼åº¦ - ä½¿ç”¨çº¯PyTorchç‰ˆæœ¬é¿å…numpyè½¬æ¢"""
        from .utils import compute_image_similarity_torch
        
        # ç»Ÿä¸€è½¬æ¢ä¸ºtorch.Tensor
        if not isinstance(img1, torch.Tensor):
            img1 = torch.from_numpy(img1).float()
        
        if not isinstance(img2, torch.Tensor):
            import numpy as np
            img2 = torch.from_numpy(np.array(img2)).float()
        
        return compute_image_similarity_torch(img1, img2) 

    def _tensor_to_numpy(self, tensor: torch.Tensor) -> np.ndarray:
        """å°†torch.Tensorè½¬æ¢ä¸ºnumpyæ•°ç»„ä¾›å¯è§†åŒ–ä½¿ç”¨"""
        if tensor is None:
            return None
        
        if isinstance(tensor, torch.Tensor):
            img_np = tensor.detach().cpu().numpy()
        else:
            img_np = np.array(tensor)
        
        # ç¡®ä¿æ˜¯HWCæ ¼å¼
        if img_np.ndim == 4:
            img_np = img_np.squeeze(0)
        
        if img_np.shape[0] == 3:  # CHW -> HWC
            img_np = img_np.transpose(1, 2, 0)
        
        # æ™ºèƒ½å¤„ç†æ•°æ®ç±»å‹å’Œæ•°å€¼èŒƒå›´
        if img_np.dtype == np.uint8:
            # å·²ç»æ˜¯uint8ï¼Œç›´æ¥ä½¿ç”¨
            return img_np
        else:
            # æ£€æŸ¥æ•°å€¼èŒƒå›´æ¥å†³å®šå¦‚ä½•è½¬æ¢
            img_min, img_max = img_np.min(), img_np.max()
            
            if img_max <= 1.0:
                # æ•°å€¼åœ¨[0,1]èŒƒå›´ï¼Œéœ€è¦ä¹˜ä»¥255
                img_np = img_np * 255.0
            elif img_max <= 255.0:
                # æ•°å€¼åœ¨[0,255]èŒƒå›´ï¼Œç›´æ¥ä½¿ç”¨
                pass
            else:
                # æ•°å€¼è¶…å‡º255ï¼Œéœ€è¦å½’ä¸€åŒ–åˆ°[0,255]
                img_np = (img_np - img_min) / (img_max - img_min) * 255.0
            
            # ç¡®ä¿ä¸¥æ ¼çš„[0,255]èŒƒå›´å¹¶è½¬æ¢ä¸ºuint8
            # ä½¿ç”¨æ›´ä¸¥æ ¼çš„èˆå…¥å’Œclippingæ¥é¿å…ç²¾åº¦é—®é¢˜
            img_np = np.clip(np.round(img_np), 0, 255).astype(np.uint8)
            
            # é¢å¤–æ£€æŸ¥ï¼šç¡®ä¿æ²¡æœ‰è¶…å‡ºèŒƒå›´çš„å€¼
            if img_np.min() < 0 or img_np.max() > 255:
                # å¦‚æœä»ç„¶è¶…å‡ºèŒƒå›´ï¼Œè¿›è¡Œå¼ºåˆ¶å½’ä¸€åŒ–
                img_np = np.clip(img_np, 0, 255)
        
        return img_np 