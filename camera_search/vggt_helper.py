"""
VGGTåŠ©æ‰‹æ¨¡å— - åŸºäºV2M4å®ç°ï¼Œä½¿ç”¨çœŸå®çš„VGGTæ¨¡å‹
"""

import numpy as np
import torch
import torch.nn.functional as F
from dataclasses import dataclass
from typing import List, Optional
from pathlib import Path
import cv2

@dataclass
class VGGTResult:
    """VGGTæ¨ç†ç»“æœ - ä¸DUSt3Rç»“æœæ¥å£å…¼å®¹"""
    reference_pc: np.ndarray           # å‚è€ƒå›¾åƒç‚¹äº‘
    rendered_pcs: List[np.ndarray]     # æ¸²æŸ“å›¾åƒç‚¹äº‘åˆ—è¡¨
    depth_maps: List[np.ndarray]       # æ·±åº¦å›¾åˆ—è¡¨
    confidence_scores: List[float]     # ç½®ä¿¡åº¦åˆ†æ•°

class VGGTHelper:
    """VGGTåŠ©æ‰‹ç±» - åŸºäºV2M4å®ç°ï¼Œä½¿ç”¨çœŸå®çš„VGGTæ¨¡å‹"""
    
    def __init__(self, device: str = "cuda"):
        self.device = device
        self.model = None
        self.is_loaded = False
        self.dtype = torch.bfloat16 if torch.cuda.get_device_capability()[0] >= 8 else torch.float16
        
    def load_model(self):
        """åŠ è½½çœŸå®çš„VGGTæ¨¡å‹ - åŸºäºV2M4å®ç° (æ— å¼‚å¸¸å¤„ç†ç‰ˆæœ¬)"""
        print("ğŸ”„ æ­£åœ¨åŠ è½½çœŸå®çš„VGGTæ¨¡å‹...")
        
        # ä½¿ç”¨V2M4ä¸­çš„æ­£ç¡®åŠ è½½æ–¹å¼ - ç›´æ¥æ‰§è¡Œï¼Œä¸æ•è·å¼‚å¸¸
        from .vggt import VGGT
        self.model = VGGT.from_pretrained("facebook/VGGT-1B").to(self.device)
        
        print("âœ… VGGTæ¨¡å‹åŠ è½½æˆåŠŸ")
        self.is_loaded = True
        
    def _preprocess_images(self, images: List[np.ndarray]) -> torch.Tensor:
        """é¢„å¤„ç†å›¾åƒ - è½¬æ¢ä¸ºVGGTæ ¼å¼ (åŸºäºV2M4å®ç°)"""
        processed_images = []
        
        # VGGTçš„æ ‡å‡†è¾“å…¥å°ºå¯¸
        target_size = 518  # ä¸V2M4ä¿æŒä¸€è‡´
        
        for img in images:
            # ç¡®ä¿å›¾åƒæ˜¯RGBæ ¼å¼
            if len(img.shape) == 3 and img.shape[2] == 3:
                # è½¬æ¢ä¸ºtensorå¹¶å½’ä¸€åŒ–åˆ°[0,1]
                if img.dtype == np.uint8:
                    img_tensor = torch.from_numpy(img).float() / 255.0
                else:
                    img_tensor = torch.from_numpy(img).float()
                
                # è°ƒæ•´ç»´åº¦é¡ºåº: HWC -> CHW
                img_tensor = img_tensor.permute(2, 0, 1)
                
                # è°ƒæ•´å°ºå¯¸åˆ°target_size
                height, width = img_tensor.shape[1], img_tensor.shape[2]
                
                # ä¿æŒå®½é«˜æ¯”è°ƒæ•´åˆ°target_size
                if height != target_size or width != target_size:
                    img_tensor = F.interpolate(
                        img_tensor.unsqueeze(0),
                        size=(target_size, target_size),
                        mode='bicubic',
                        align_corners=False
                    ).squeeze(0)
                
                processed_images.append(img_tensor)
        
        # å †å æˆbatch: [S, 3, H, W]
        return torch.stack(processed_images).to(self.device)
    
    def _extract_point_clouds(self, predictions: dict, images: torch.Tensor) -> List[np.ndarray]:
        """ä»VGGTé¢„æµ‹ç»“æœä¸­æå–ç‚¹äº‘ - åŸºäºV2M4å®ç°"""
        point_clouds = []
        
        # è·å–ä¸–ç•Œåæ ‡ç‚¹äº‘ - ä¸V2M4ä¿æŒä¸€è‡´çš„å¤„ç†æ–¹å¼
        world_points = predictions["world_points"][0].detach()  # [S, H, W, 3]
        
        # æ’å€¼åˆ°åŸå›¾åƒå°ºå¯¸
        world_points = F.interpolate(
            world_points.permute(0, 3, 1, 2),  # [S, H, W, 3] -> [S, 3, H, W]
            size=images.shape[-1],
            mode='bilinear', 
            align_corners=False
        ).permute(0, 2, 3, 1)  # [S, 3, H', W'] -> [S, H', W', 3]
        
        for i, pts in enumerate(world_points):
            # æå–æœ‰æ•ˆç‚¹äº‘ (éé»‘è‰²åƒç´ åŒºåŸŸ) - ä¸V2M4ä¿æŒä¸€è‡´
            valid_mask = images[i].permute(1, 2, 0).sum(-1) > 0  # [H, W]
            points = pts.view(-1, 3)[valid_mask.view(-1)]  # é€‰æ‹©æœ‰æ•ˆç‚¹
            
            # è½¬æ¢ä¸ºnumpy
            point_clouds.append(points.cpu().numpy())
        
        return point_clouds
    
    def inference(self, reference_image: np.ndarray, rendered_views: List[np.ndarray]) -> VGGTResult:
        """VGGTæ¨ç† - ä½¿ç”¨çœŸå®çš„VGGTæ¨¡å‹ (æ— å¼‚å¸¸å¤„ç†ç‰ˆæœ¬)"""
        
        if not self.is_loaded:
            self.load_model()
            
        # ç›´æ¥æ‰§è¡Œï¼Œä¸æ£€æŸ¥æ¨¡å‹çŠ¶æ€
        # 1. é¢„å¤„ç†å›¾åƒ
        all_images = [reference_image] + rendered_views
        images_tensor = self._preprocess_images(all_images)
        
        # 2. VGGTæ¨ç† - ä½¿ç”¨ä¸V2M4ç›¸åŒçš„æ–¹å¼
        with torch.no_grad():
            with torch.cuda.amp.autocast(dtype=self.dtype):
                predictions = self.model(images_tensor)
        
        # 3. æå–ç‚¹äº‘
        point_clouds = self._extract_point_clouds(predictions, images_tensor)
        
        # 4. æ„å»ºç»“æœ
        reference_pc = point_clouds[0] if len(point_clouds) > 0 else np.array([]).reshape(0, 3)
        rendered_pcs = point_clouds[1:] if len(point_clouds) > 1 else []
        
        # æå–æ·±åº¦å›¾
        depth_maps = []
        if "depth" in predictions:
            depth_tensor = predictions["depth"][0].detach()  # [S, H, W, 1]
            for i in range(depth_tensor.shape[0]):
                depth_map = depth_tensor[i, :, :, 0].cpu().numpy()
                depth_maps.append(depth_map)
        
        # è®¡ç®—ç½®ä¿¡åº¦åˆ†æ•°
        confidence_scores = []
        if "world_points_conf" in predictions:
            conf_tensor = predictions["world_points_conf"][0].detach()  # [S, H, W]
            for i in range(conf_tensor.shape[0]):
                avg_conf = conf_tensor[i].mean().item()
                confidence_scores.append(avg_conf)
        else:
            confidence_scores = [0.8 for _ in rendered_pcs]
        
        return VGGTResult(
            reference_pc=reference_pc,
            rendered_pcs=rendered_pcs,
            depth_maps=depth_maps,
            confidence_scores=confidence_scores
        ) 