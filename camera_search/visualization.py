"""
å¯è§†åŒ–æ¨¡å—
ä¸ºV2M4ç›¸æœºæœç´¢ç®—æ³•æä¾›ç»“æœå¯è§†åŒ–åŠŸèƒ½
"""

import os
import numpy as np
import cv2
import matplotlib.pyplot as plt
from pathlib import Path
from datetime import datetime
from typing import Optional, List, Dict, Any
from PIL import Image
import trimesh

from .core import CameraPose, DataPair
from .utils import compute_image_similarity

class V2M4Visualizer:
    """V2M4ç®—æ³•ç»“æœå¯è§†åŒ–å™¨"""
    
    def __init__(self, output_dir: str = "outputs/visualization"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
    def create_result_comparison(self, 
                               data_pair: DataPair,
                               reference_image: np.ndarray,
                               rendered_result: Optional[np.ndarray],
                               final_pose: CameraPose,
                               mesh_info: Optional[Dict] = None,
                               algorithm_stats: Optional[Dict] = None,
                               execution_time: float = 0.0) -> str:
        """åˆ›å»ºç»“æœå¯¹æ¯”å›¾"""
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        comparison_path = self.output_dir / f"v2m4_result_{data_pair.scene_name}_{timestamp}.png"
        
        # åˆ›å»º3åˆ—å¸ƒå±€çš„å¯¹æ¯”å›¾
        fig, axes = plt.subplots(1, 3, figsize=(18, 6))
        
        # ç¬¬1åˆ—ï¼šå‚è€ƒå›¾åƒ
        axes[0].imshow(reference_image)
        axes[0].set_title("Reference Image", fontsize=14, fontweight='bold')
        axes[0].axis('off')
        
        # ç¬¬2åˆ—ï¼šæ¸²æŸ“ç»“æœ
        if rendered_result is not None:
            axes[1].imshow(rendered_result)
            
            # è®¡ç®—ç›¸ä¼¼æ€§æŒ‡æ ‡
            similarity = compute_image_similarity(reference_image, rendered_result)
            
            title = f"V2M4 Estimated Pose\n(Similarity: {similarity:.3f})"
            title_color = 'green' if similarity > 0.7 else 'orange' if similarity > 0.5 else 'red'
        else:
            placeholder = np.full_like(reference_image, 128, dtype=np.uint8)
            axes[1].imshow(placeholder)
            title = "V2M4 Estimated Pose\n(Rendering Failed)"
            title_color = 'red'
        
        axes[1].set_title(title, fontsize=14, fontweight='bold', color=title_color)
        axes[1].axis('off')
        
        # ç¬¬3åˆ—ï¼šè¯¦ç»†ä¿¡æ¯
        info_text = self._generate_info_text(
            data_pair, final_pose, mesh_info, algorithm_stats, execution_time
        )
        
        axes[2].text(0.05, 0.95, info_text, fontsize=10, 
                    transform=axes[2].transAxes, verticalalignment='top',
                    fontfamily='monospace', bbox=dict(boxstyle="round,pad=0.3", 
                    facecolor="lightgray", alpha=0.8))
        axes[2].set_xlim(0, 1)
        axes[2].set_ylim(0, 1)
        axes[2].axis('off')
        axes[2].set_title("Algorithm Details", fontsize=14, fontweight='bold')
        
        plt.tight_layout()
        plt.savefig(comparison_path, dpi=150, bbox_inches='tight', facecolor='white')
        plt.close()
        
        print(f"ğŸ“Š å¯è§†åŒ–ç»“æœå·²ä¿å­˜: {comparison_path}")
        return str(comparison_path)
    
    def create_pose_progression_visualization(self,
                                            data_pair: DataPair,
                                            reference_image: np.ndarray,
                                            progression_data: List[Dict],
                                            final_pose: CameraPose) -> str:
        """åˆ›å»ºå§¿æ€ä¼˜åŒ–è¿‡ç¨‹å¯è§†åŒ–"""
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        progression_path = self.output_dir / f"v2m4_progression_{data_pair.scene_name}_{timestamp}.png"
        
        # å‡†å¤‡æ•°æ®
        num_steps = len(progression_data)
        if num_steps == 0:
            return ""
        
        # åˆ›å»ºå¤šè¡Œå¸ƒå±€
        rows = 2
        cols = min(4, num_steps + 1)  # æœ€å¤š4åˆ—ï¼ŒåŒ…æ‹¬å‚è€ƒå›¾åƒ
        
        fig, axes = plt.subplots(rows, cols, figsize=(cols * 4, rows * 3))
        if rows == 1:
            axes = axes.reshape(1, -1)
        
        # ç¬¬ä¸€å¼ å›¾ï¼šå‚è€ƒå›¾åƒ
        axes[0, 0].imshow(reference_image)
        axes[0, 0].set_title("Reference Image", fontsize=12, fontweight='bold')
        axes[0, 0].axis('off')
        
        # æ˜¾ç¤ºä¼˜åŒ–æ­¥éª¤
        for i, step_data in enumerate(progression_data[:cols-1]):
            col_idx = i + 1
            
            # ä¸Šæ’ï¼šæ¸²æŸ“ç»“æœ
            if 'rendered_image' in step_data and step_data['rendered_image'] is not None:
                axes[0, col_idx].imshow(step_data['rendered_image'])
            else:
                placeholder = np.full_like(reference_image, 128, dtype=np.uint8)
                axes[0, col_idx].imshow(placeholder)
            
            step_name = step_data.get('step_name', f'Step {i+1}')
            similarity = step_data.get('similarity', 0.0)
            axes[0, col_idx].set_title(f"{step_name}\nSim: {similarity:.3f}", 
                                      fontsize=10, fontweight='bold')
            axes[0, col_idx].axis('off')
            
            # ä¸‹æ’ï¼šå§¿æ€å‚æ•°
            pose = step_data.get('pose')
            if pose:
                param_text = f"""Pose Parameters:
Elevation: {pose.elevation:.1f}Â°
Azimuth: {pose.azimuth:.1f}Â°
Distance: {pose.radius:.2f}
Center: ({pose.center_x:.2f}, {pose.center_y:.2f}, {pose.center_z:.2f})

Score: {step_data.get('score', 'N/A')}"""
            else:
                param_text = "No pose data available"
            
            axes[1, col_idx].text(0.1, 0.9, param_text, fontsize=8,
                                 transform=axes[1, col_idx].transAxes, 
                                 verticalalignment='top', fontfamily='monospace')
            axes[1, col_idx].set_xlim(0, 1)
            axes[1, col_idx].set_ylim(0, 1)
            axes[1, col_idx].axis('off')
        
        # éšè—å¤šä½™çš„å­å›¾
        for i in range(len(progression_data) + 1, cols):
            axes[0, i].axis('off')
            axes[1, i].axis('off')
        
        # ä¸‹æ’ç¬¬ä¸€ä¸ªä½ç½®ï¼šç®—æ³•æ€»ç»“
        summary_text = f"""V2M4 Algorithm Summary:
Scene: {data_pair.scene_name}
Total Steps: {num_steps}

Final Pose:
â€¢ Elevation: {final_pose.elevation:.2f}Â°
â€¢ Azimuth: {final_pose.azimuth:.2f}Â°
â€¢ Distance: {final_pose.radius:.3f}

Optimization Progress:
{' â†’ '.join([step.get('step_name', f'S{i+1}') for i, step in enumerate(progression_data)])}"""
        
        axes[1, 0].text(0.1, 0.9, summary_text, fontsize=9,
                       transform=axes[1, 0].transAxes, verticalalignment='top',
                       fontfamily='monospace', bbox=dict(boxstyle="round,pad=0.3", 
                       facecolor="lightblue", alpha=0.8))
        axes[1, 0].set_xlim(0, 1)
        axes[1, 0].set_ylim(0, 1)
        axes[1, 0].axis('off')
        
        plt.tight_layout()
        plt.savefig(progression_path, dpi=150, bbox_inches='tight', facecolor='white')
        plt.close()
        
        print(f"ğŸ“ˆ ä¼˜åŒ–è¿‡ç¨‹å¯è§†åŒ–å·²ä¿å­˜: {progression_path}")
        return str(progression_path)
    
    def create_batch_results_summary(self, 
                                   batch_results: Dict[str, Any],
                                   execution_times: Dict[str, float]) -> str:
        """åˆ›å»ºæ‰¹é‡å¤„ç†ç»“æœæ€»ç»“"""
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        summary_path = self.output_dir / f"v2m4_batch_summary_{timestamp}.png"
        
        # ç»Ÿè®¡æ•°æ®
        total_scenes = len(batch_results)
        successful_scenes = sum(1 for result in batch_results.values() if result is not None)
        success_rate = (successful_scenes / total_scenes) * 100 if total_scenes > 0 else 0
        
        avg_time = np.mean(list(execution_times.values())) if execution_times else 0
        total_time = sum(execution_times.values()) if execution_times else 0
        
        # åˆ›å»ºå›¾è¡¨
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
        
        # 1. æˆåŠŸç‡é¥¼å›¾
        success_data = [successful_scenes, total_scenes - successful_scenes]
        success_labels = ['Successful', 'Failed']
        success_colors = ['lightgreen', 'lightcoral']
        
        ax1.pie(success_data, labels=success_labels, colors=success_colors, autopct='%1.1f%%')
        ax1.set_title(f'Success Rate: {success_rate:.1f}%\n({successful_scenes}/{total_scenes} scenes)', 
                     fontsize=14, fontweight='bold')
        
        # 2. æ‰§è¡Œæ—¶é—´åˆ†å¸ƒ
        if execution_times:
            times = list(execution_times.values())
            ax2.hist(times, bins=10, color='skyblue', alpha=0.7, edgecolor='black')
            ax2.set_xlabel('Execution Time (seconds)')
            ax2.set_ylabel('Number of Scenes')
            ax2.set_title(f'Execution Time Distribution\nAvg: {avg_time:.1f}s, Total: {total_time:.1f}s', 
                         fontsize=14, fontweight='bold')
            ax2.grid(True, alpha=0.3)
        
        # 3. åœºæ™¯ç»“æœåˆ—è¡¨
        scene_names = list(batch_results.keys())
        scene_statuses = ['âœ…' if batch_results[name] is not None else 'âŒ' for name in scene_names]
        scene_times = [execution_times.get(name, 0) for name in scene_names]
        
        # åˆ›å»ºè¡¨æ ¼
        table_data = []
        for i, (name, status, time_val) in enumerate(zip(scene_names, scene_statuses, scene_times)):
            table_data.append([f"{i+1:2d}", name, status, f"{time_val:.1f}s"])
        
        table = ax3.table(cellText=table_data, 
                         colLabels=['#', 'Scene Name', 'Status', 'Time'],
                         cellLoc='left', loc='center')
        table.auto_set_font_size(False)
        table.set_fontsize(9)
        table.scale(1, 1.5)
        
        # è®¾ç½®è¡¨æ ¼æ ·å¼
        for (i, j), cell in table.get_celld().items():
            if i == 0:  # æ ‡é¢˜è¡Œ
                cell.set_facecolor('#4CAF50')
                cell.set_text_props(weight='bold', color='white')
            elif j == 2:  # çŠ¶æ€åˆ—
                if i > 0:
                    cell.set_facecolor('#E8F5E8' if 'âœ…' in cell.get_text().get_text() else '#FFE8E8')
        
        ax3.axis('off')
        ax3.set_title('Individual Scene Results', fontsize=14, fontweight='bold')
        
        # 4. æ€»ç»“ä¿¡æ¯
        summary_info = f"""V2M4 Batch Processing Summary
{'='*40}

ğŸ“Š Overall Statistics:
â€¢ Total Scenes: {total_scenes}
â€¢ Successful: {successful_scenes} ({success_rate:.1f}%)
â€¢ Failed: {total_scenes - successful_scenes}

â±ï¸ Performance:
â€¢ Total Time: {total_time:.1f} seconds
â€¢ Average Time: {avg_time:.1f} seconds per scene
â€¢ Fastest: {min(execution_times.values()) if execution_times else 0:.1f}s
â€¢ Slowest: {max(execution_times.values()) if execution_times else 0:.1f}s

ğŸ¯ Algorithm Configuration:
â€¢ Initial Samples: 2000
â€¢ Top-N Selection: 7
â€¢ PSO Particles: 50
â€¢ PSO Iterations: 20
â€¢ Gradient Descent: 100 iterations

Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"""
        
        ax4.text(0.05, 0.95, summary_info, fontsize=10,
                transform=ax4.transAxes, verticalalignment='top',
                fontfamily='monospace', bbox=dict(boxstyle="round,pad=0.5", 
                facecolor="lightyellow", alpha=0.9))
        ax4.set_xlim(0, 1)
        ax4.set_ylim(0, 1)
        ax4.axis('off')
        
        plt.tight_layout()
        plt.savefig(summary_path, dpi=150, bbox_inches='tight', facecolor='white')
        plt.close()
        
        print(f"ğŸ“‹ æ‰¹é‡å¤„ç†æ€»ç»“å·²ä¿å­˜: {summary_path}")
        return str(summary_path)
    
    def save_individual_results(self,
                               data_pair: DataPair,
                               reference_image: np.ndarray,
                               rendered_result: Optional[np.ndarray]) -> List[str]:
        """ä¿å­˜å•ç‹¬çš„ç»“æœå›¾åƒ"""
        
        saved_paths = []
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ä¿å­˜å‚è€ƒå›¾åƒ
        ref_path = self.output_dir / f"reference_{data_pair.scene_name}_{timestamp}.png"
        Image.fromarray(reference_image).save(ref_path)
        saved_paths.append(str(ref_path))
        
        # ä¿å­˜æ¸²æŸ“ç»“æœ
        if rendered_result is not None:
            render_path = self.output_dir / f"rendered_{data_pair.scene_name}_{timestamp}.png"
            Image.fromarray(rendered_result).save(render_path)
            saved_paths.append(str(render_path))
        
        return saved_paths
    
    def _generate_info_text(self, 
                           data_pair: DataPair,
                           final_pose: CameraPose,
                           mesh_info: Optional[Dict],
                           algorithm_stats: Optional[Dict],
                           execution_time: float) -> str:
        """ç”Ÿæˆè¯¦ç»†ä¿¡æ¯æ–‡æœ¬"""
        
        info_lines = [
            "ğŸ¯ V2M4 Camera Search Results",
            "=" * 35,
            "",
            f"ğŸ“ Scene: {data_pair.scene_name}",
            f"ğŸ•’ Time: {execution_time:.1f}s",
            "",
            "ğŸ“ Final Camera Pose:",
            f"â€¢ Elevation: {final_pose.elevation:.2f}Â°",
            f"â€¢ Azimuth: {final_pose.azimuth:.2f}Â°", 
            f"â€¢ Distance: {final_pose.radius:.3f}",
            f"â€¢ Center: ({final_pose.center_x:.2f}, {final_pose.center_y:.2f}, {final_pose.center_z:.2f})",
            ""
        ]
        
        # æ·»åŠ meshä¿¡æ¯
        if mesh_info:
            info_lines.extend([
                "ğŸ”º Mesh Information:",
                f"â€¢ Vertices: {mesh_info.get('vertices_count', 'N/A')}",
                f"â€¢ Faces: {mesh_info.get('faces_count', 'N/A')}",
                f"â€¢ Scale: {mesh_info.get('scale', 0):.2f}",
                ""
            ])
        
        # æ·»åŠ ç®—æ³•ç»Ÿè®¡
        if algorithm_stats:
            info_lines.extend([
                "âš™ï¸ Algorithm Stats:",
                f"â€¢ Initial Samples: {algorithm_stats.get('initial_samples', 'N/A')}",
                f"â€¢ Top-N Selected: {algorithm_stats.get('top_n', 'N/A')}",
                f"â€¢ PSO Iterations: {algorithm_stats.get('pso_iterations', 'N/A')}",
                f"â€¢ Final Score: {algorithm_stats.get('final_score', 'N/A'):.4f}",
                ""
            ])
        
        # æ·»åŠ ç³»ç»Ÿä¿¡æ¯
        import torch
        info_lines.extend([
            "ğŸ’» System:",
            f"â€¢ GPU: {'Available' if torch.cuda.is_available() else 'Not Available'}",
            f"â€¢ Generated: {datetime.now().strftime('%H:%M:%S')}"
        ])
        
        return "\n".join(info_lines) 